---
title: "Lab3"
output: html_document
date: "2022-11-26"
---

# Lab3

```{r}
TIN <- 20

set.seed(TIN)
```

## Part I: Markov Chain

### Problem 1

```{r}
TLN <- paste("0", TIN, sep = "")
```

Transition probabilities matrix:

$$
P=\begin{bmatrix}
0.9 & 0.1 & 0 & 0\\
0.8 & 0.1 & 0.1 & 0\\
0.9 & 0 & 0 & 0.1\\
0.8 & 0.1 & 0.1 & 0\\
\end{bmatrix}
$$

Calculating limiting probabilities with two methods (using eigenvectors and multiplying matrices):

```{r}
P = matrix(c(
  0.9, 0.1, 0.0, 0.0,
  0.8, 0.1, 0.1, 0.0,
  0.9, 0.0, 0.0, 0.1,
  0.8, 0.1, 0.1, 0), nrow=4, byrow=T)

calculate_by_eigenvector <- function (probs) {
  left_eigenvector <- eigen(t(probs))$vectors[,1]
  return (sapply(left_eigenvector / sum(left_eigenvector), Re))
}

calculate_by_matrix_multiplication <- function (probs) {
  return ((probs %*% probs %*% probs %*% probs)[1, ])
}

cat("Limiting probabilities (eigenvectors): ", calculate_by_eigenvector(P), "\n")
cat("Limiting probabilities (matrices): ", calculate_by_matrix_multiplication(P))

```

Here are two functions for estimating theoretical and numerical probabilities of occurrence of TLN in the randomly generated digit sequence:

```{r}
estimate_probability <- Vectorize(function (m, n) {
  sample_data <- apply(
    replicate(m, sample(0:9, n, replace = T)), 2,
    function(x) grepl(TLN, paste(x, collapse = "")))
  return (length(sample_data[sample_data == TRUE]) / length(sample_data))
})

theoretical_probability <- function (n) 1 - ((1 - 0.001) ^ n)
# 0.001 is the limiting probability of the state 4
```

Calculating these probabilities for different sample sizes:

```{r}
x <- seq(100, 2000, by = 100)
estimated_data = estimate_probability(500, x)
theoretical_data = theoretical_probability(x)
```

```{r}
plot(
  x,
  estimated_data, col ="blue", lwd = 3)
lines(
  x,
  theoretical_data)
```

Since $\bar{X}$ is our sample mean $M_n$ and $\mu$ is our theoretical mean, let $Z_n := \frac{\sqrt{n}}{\sigma}(M_n-\mu)$ be a random variable and, according to the Central Limit Theorem, it converges in law to the standard Gaussian random variable $Z \sim N(0,1)$ . Since we need to find a sample size $N$ which would guarantee that the absolute error of the estimate is below $0.03$ with confidence level of at least $95$ percent, we obtain the next equation:$$
P(|\bar{X}-\mu|\leq\varepsilon)=1-2\Phi(-\frac{\sqrt{n}}{\sigma}\varepsilon) \\
1-2\Phi(-\frac{\sqrt{n}}{\sigma}\varepsilon)=0.95 \\
\Phi(-\frac{\sqrt{n}}{\sigma}\varepsilon)=0.025 \\
\sqrt{n}=-\frac{\sigma\cdot z_{0.025}}{\varepsilon} \\
n=\sigma^2(\frac{z_{0.025}}{\varepsilon})^2 \\
n=p_n\cdot(1-p_n)\cdot(\frac{z_{0.025}}{0.03})^2 \\
n=(1-0.999^{1000})\cdot(0.999^{1000})\cdot(\frac{z_{0.025}}{0.03})^2 \\
n\sim992 
$$

```{r}
x <- seq(300, 800, by = 100)
n <- 1000
m <- 100
estimated_data = apply(replicate(m, estimate_probability(x, n) - theoretical_probability(n)), 1, function (x) quantile(x, .95))
plot(
  x,
  estimated_data, col ="blue", lwd = 3, type="l")
abline(h = 0.03, col = "darkgreen")
```

```{r}
N <- 1000
n <- 1000
m <- 100
hist(
  replicate(m, abs(estimate_probability(N, n) - theoretical_probability(n))),
  main = "Absolute error histogram",
  xlab = "Absolute error")
```

### Problem 2

Transition probabilities matrix:

$$
P=\begin{bmatrix}
0.9 & 0.1 & 0 & 0\\
0.8 & 0.1 & 0.1 & 0\\
0.9 & 0 & 0 & 0.1\\
0 & 0 & 0 & 1\\
\end{bmatrix}
$$

$$
\begin{cases}
\mu_1=1+0.9\cdot\mu_1+0.1\cdot\mu_2 \\
\mu_2=1+0.8\cdot\mu_1+0.1\cdot\mu_2+0.1\cdot\mu_3 \\
\mu_3=1+0.9\cdot\mu_1+0.1\cdot\mu_4 \\
\mu_4=0
\end{cases}
\Rightarrow
\begin{cases}
\mu_1=\frac{1+0.1\cdot\frac{1+0.8\cdot\mu_1+0.1\cdot(1+0.9\cdot\mu_1)}{0.9}}{0.1} \\
\mu_2=\frac{1+0.8\cdot\mu_1+0.1\cdot(1+0.9\cdot\mu_1)}{0.9} \\
\mu_3=1+0.9\cdot\mu_1 \\
\mu_4=0
\end{cases}
\\
\begin{cases}
\mu_1=10+\frac{10}{9}+\frac{8}{9}\mu_1+\frac{1}{9}+\frac{1}{10}\mu_1\Rightarrow\frac{90-80-9}{90}\mu_1=\frac{101}{9}\Rightarrow\mu_1=1010 \\
\mu_2=\frac{1+0.8\cdot\mu_1+0.1\cdot(1+0.9\cdot\mu_1)}{0.9}=\frac{10}{9}+\frac{8}{9}\mu_1+\frac{1}{9}+\frac{1}{10}\mu_1=\frac{11}{9}+\frac{89}{90}\mu_1=1000 \\
\mu_3=1+0.9\cdot\mu_1=1+909=910 \\
\mu_4=0
\end{cases}
$$

Hence, the estimated length is $\mu_1=1010$

Calculating expected time till absorption numerically:

```{r}
calculate_length <- Vectorize(function() {
  values <- vector()
  length <- 0
  while (paste(values, collapse = "") != TLN) {
    values <- tail(append(values, sample(0:9, 1)), nchar(TLN))
    length <- length + 1
  }
  return (length)
})

data <- replicate(100, calculate_length())
mean(data)
```

Now, let's find the sample size $N$ which guarantees that the absolute error $|\hat{\theta} − \theta|$ of the estimate does not exceed 10 with confidence level of at least $95$ percent. By Chebyshev inequality:

$$
P(|\hat{\theta}-\theta|<k\sigma)=1-
P(|\hat{\theta}-\theta|\geq k\sigma)\geq1-\frac{\sigma^2}{\sigma^2k^2}=1-\frac{1}{k^2} \\
1-\frac{1}{k^2}=0.95\Rightarrow k^2=20 \\
\sigma=\frac{\sigma_S}{\sqrt{n}} \\
k\sigma=10\Rightarrow\frac{\sigma_S}{\sqrt{n}}=\frac{10}{k}\Rightarrow n=\frac{\sigma_S^2\cdot k^2}{100}=\frac{\sigma_S^2}{5}
$$

Now, knowing how to find the needed sample size, we can numerically calculate variance and use it in the formula above:

```{r}
var(data) / 5
```

## Part II: Parameter estimation

### Problem 3

```{r}
theta <- TIN / 10
m <- 3000
x <- seq(100, 500, by=100)


estimate_method_1 <- function(data, alpha) {
  n <- length(data)
  lower = (2 * sum(data)) / (qchisq(df = 2 * n, p = 1 - alpha / 2))
  upper = (2 * sum(data)) / (qchisq(df = 2 * n, p = alpha / 2))
  return (c(lower, upper))
}


estimate_method_2 <- function(data, alpha) {
  n <- length(data)
  lower = mean(data) - (theta * qnorm(1 - alpha / 2)) / sqrt(n)
  upper = mean(data) + (theta * qnorm(1 - alpha / 2)) / sqrt(n)
  return (c(lower, upper))
}


estimate_method_3 <- function(data, alpha) {
  n <- length(data)
  lower = (mean(data) * sqrt(n)) / (qnorm(1 - alpha / 2) + sqrt(n))
  upper = (mean(data) * sqrt(n)) / (qnorm(alpha / 2) + sqrt(n))
  return (c(lower, upper))
}


estimate_method_4 <- function(data, alpha) {
  n <- length(data)
  lower = mean(data) - qt(1 - alpha / 2, n - 1) * sd(data) / sqrt(n)
  upper = mean(data) + qt(1 - alpha / 2, n - 1) * sd(data) / sqrt(n)
  return (c(lower, upper))
}


calculate_percentage <- function(n, alpha, method, generation_method) {
  func <- function (data) method(data, alpha)
  gen_method <- if (generation_method == "rexp") rexp else rpois
  mn <- if (generation_method == "rexp") 1 / theta else theta
  sample_data <- apply(matrix(gen_method(n * m, mn), nrow = m), 1, func)
  return (length(sample_data[,sample_data[1,] < theta & theta < sample_data[2,]]) / length(sample_data))
}


plot_method <- function(n, alpha, method, generation_method) {
  func <- Vectorize(function (n) calculate_percentage(n, alpha, method, generation_method))
  plot(
    x,
    func(x),
    type = "l",
    main = paste("Alpha is", alpha)
  )
}


plot_all <- function (method, generation_method) {
  plot_method(x, .1, method, generation_method)
  plot_method(x, .05, method, generation_method)
  plot_method(x, .01, method, generation_method)
}
```

#### Method 1

Let $Y_k=2\lambda X_k$. Hence $F_{Y_k}=P(Y_k\leq x)=P(2\lambda X_k\leq x)=P(X_k\leq\frac{x}{2\lambda})=F_{X_k}(\frac{x}{2\lambda})$

Taking the derivative on both sides:$f_{Y_{k}}( x) =\frac{1}{2\lambda } f_{X_{k}}( x) =\frac{1}{2\lambda } \lambda e^{-\lambda \frac{x}{2\lambda }} =\frac{1}{2} e^{-\frac{1}{2} x}$. Therefore, $Y_{k} \sim \mathcal{E}\left(\frac{1}{2}\right)$

Let's introduce statistics $T=2\lambda n\overline{X} =2\lambda \cdotp ( X_{1} +…+X_{n}) =Y_{1} +…+Y_{n}$.

Hence, $T\sim \Gamma \left( n,\ \frac{1}{2}\right) \sim \chi _{2n}^{2}$

Then, $P(\chi^{2n}_{\alpha/2}\leq2\lambda n\bar X\leq\chi^{2n}_{1-\alpha/2})=1-\alpha/2-\alpha/2=1-\alpha$.

$P(\chi^{2n}_{\alpha/2}\leq2\lambda n\bar X\leq\chi^{2n}_{1-\alpha/2})=P(\frac{\chi^{2n}_{\alpha/2}}{2\sum^n_{i=1}X_i}\leq\lambda \leq\frac{\chi^{2n}_{1-\alpha/2}}{2\sum^n_{i=1}X_i})=P(\frac{2\sum^n_{i=1}X_i}{\chi^{2n}_{1-\alpha/2}}\leq\theta \leq\frac{2\sum^n_{i=1}X_i}{\chi^{2n}_{\alpha/2}})=1-\alpha$

Therefore, $[\frac{2\sum^n_{i=1}X_i}{\chi^{2n}_{1-\alpha/2}},\frac{2\sum^n_{i=1}X_i}{\chi^{2n}_{\alpha/2}}]$ is $100(1-\alpha)$ confidence interval for $\theta$

```{r}
plot_all(estimate_method_1, "rexp")
```

#### Method 2

Here we form the Z-statistics $Z:= \frac{\sqrt{n}  \cdot (\bar{X} - \theta) }{(\theta) } \sim N(0 , 1)$ to find that:

$$
P(|\theta - \bar{X}| \le \frac{z_{\beta} \cdot \theta}{\sqrt{n}}) = P(|Z| \le z_{\beta}) = 2\beta-1
$$

We devide everything by $\theta$. Then we open our module , then rearrange the left part. In the end we multiply by $-1$ and multiply everything by $\bar{X}$):

$$
Z:= \frac{\sqrt{n}  \cdot (\bar{X} - \theta) }{(\theta) } \sim N(0 , 1)\\1- \alpha = P((|\bar{X} - \theta|) \le \frac{\theta \cdot z_{1 - \frac{\alpha}{2}}} {\sqrt{n}}) = P((|1 - \frac{\bar{X}}{\theta}|) \le \frac{ z_{1 - \frac{\alpha}{2}}} {\sqrt{n}}) \\= P(1 - \frac{z_{1 - \frac{\alpha}{2}}}{\sqrt{n}}  \le \frac{\bar{X}}{\theta} \le 1 + \frac{z_{1 - \frac{\alpha}{2}}}{\sqrt{n}}) =  P( \frac{\sqrt{n} +z_{1\frac{\alpha}{2}}}{\sqrt{n}}  \le \frac{\bar{X}}{\theta} \le  \frac{\sqrt{n} + z_{1 - \frac{\alpha}{2}}}{\sqrt{n}}) = \\ P (\frac{\sqrt{n} \cdot \bar{X}}{\sqrt{n} + z_{1 - \frac{\alpha}{2}}}  \le \theta \le  \frac{\sqrt{n} \cdot \bar{X}}{\sqrt{n} + z_{\frac{\alpha}{2}}}) \\ 1 - \alpha = [\frac{\sqrt{n} \cdot \bar{X}}{\sqrt{n} + z_{1 - \frac{\alpha}{2}}} ; \frac{\sqrt{n} \cdot \bar{X}}{\sqrt{n} + z_{\frac{\alpha}{2}}}]
$$

```{r}
plot_all(estimate_method_2, "rexp")
```

#### Method 3

Now we have to calculate confidence interval with the unknown variance $s^2 = \theta^2$ . To do that, we have to solve a double inequality:

$$
|\bar{X} - \theta| \le \frac{z_{\beta} \cdot \theta}{\sqrt{n}}
$$

$$
P(|\bar{X} - \theta| \le \frac{z_{1-\frac{\alpha}{2}} \cdot \theta}{\sqrt{n}})
$$

divide both sides by $\theta$ :

$$
P(|\frac{\bar{X}}{\theta} - 1| \le \frac{z_{1-\frac{\alpha}{2}}}{\sqrt{n}})
$$

$$
P(-\frac{z_{1-\frac{\alpha}{2}}}{\sqrt{n}} \le \frac{\bar{X}}{\theta} - 1 \le \frac{z_{1-\frac{\alpha}{2}}}{\sqrt{n}})
$$

$$
P(1-\frac{z_{1-\frac{\alpha}{2}}}{\sqrt{n}} \le \frac{\bar{X}}{\theta} \le \frac{z_{1-\frac{\alpha}{2}}}{\sqrt{n}}+1)
$$

$$
P(\frac{\sqrt{n}-z_{1-\frac{\alpha}{2}}}{\sqrt{n}} \le \frac{\theta}{\bar{X}} \le \frac{\sqrt{n}z_{1-\frac{\alpha}{2}}}{\sqrt{n}})
$$

$$
P(\bar{X} \cdot \frac{\sqrt{n}-z_{1-\frac{\alpha}{2}}}{\sqrt{n}} \le \theta \le \bar{X} \cdot \frac{\sqrt{n}z_{1-\frac{\alpha}{2}}}{\sqrt{n}})
$$

Hence, we obtain the next confidence interval of level $1-\alpha$ :

$$
[\bar{X} \cdot \frac{\sqrt{n}-z_{1-\frac{\alpha}{2}}}{\sqrt{n}};  \bar{X} \cdot \frac{\sqrt{n}z_{1-\frac{\alpha}{2}}}{\sqrt{n}}]
$$

```{r}
plot_all(estimate_method_3, "rexp")
```

#### Method 4

Let $T=(\bar X-\mu)\frac{\sqrt n}{S}\sim\mathcal{T}_{n-1}$ (as described in Ross textbook)

Hence, $P(t^{(n-1)}_{\alpha/2}\leq(\bar X-\mu)\frac{\sqrt n}{S}\leq t^{(n-1)}_{1-\alpha/2})=1-\alpha/2-\alpha/2=1-\alpha$.

$P(t^{(n-1)}_{\alpha/2}\leq(\bar X-\mu)\frac{\sqrt n}{S}\leq t^{(n-1)}_{1-\alpha/2})=P(-t^{(n-1)}_{\alpha/2}\frac{S}{\sqrt n}\geq\mu-\bar X\geq -t^{(n-1)}_{1-\alpha/2}\frac{S}{\sqrt n})=$

$=P(\bar X-t^{(n-1)}_{1-\alpha/2}\frac{S}{\sqrt n}\leq\mu\leq\bar X-t^{(n-1)}_{\alpha/2}\frac{S}{\sqrt n})=1-\alpha$

Thus, $[\bar X-t^{(n-1)}_{1-\alpha/2}\frac{S}{\sqrt n},\bar X+t^{(n-1)}_{1-\alpha/2}\frac{S}{\sqrt n}]$ is $100(1-\alpha)$ confidence interval for $\theta$

```{r}
plot_all(estimate_method_4, "rexp")
```

### Problem 4

#### Method 2

```{r}
plot_all(estimate_method_2, "rpois")
```

#### Method 3

```{r}
plot_all(estimate_method_3, "rpois")
```

#### Method 4

```{r}
plot_all(estimate_method_4, "rpois")
```

#### Conclusion

Completing this task, we have obtained a better and more deep understanding of parameter estimation topic and have also recalled Markov chains. We have also verified that the interval estimates produced by the known rules indeed contain the parameter with probability equal to the confidence level and made a conclusion, that all four methods give more or less equal evaluation of the interval, except for the case with Poisson distribution and method 3: compared to method 2 and 4 it gives a less accurate estimate.
